// lib/services/gpt4o_audio_service.dart (ê°œì„ ëœ ë²„ì „)
// ğŸš€ NEW: GPT-4o-audio-preview ì „ìš© ì„œë¹„ìŠ¤
// ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ë° ë„¤ì´í‹°ë¸Œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤
import 'dart:async';
import 'dart:convert';
import 'dart:io';
import 'package:http/http.dart' as http;
import 'package:flutter/foundation.dart';
import 'package:path_provider/path_provider.dart';
import 'package:uuid/uuid.dart';
import 'package:connectivity_plus/connectivity_plus.dart';
import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:tiiun/services/remote_config_service.dart';
import 'package:tiiun/services/openai_tts_service.dart'; // ğŸ†• TTS ì„œë¹„ìŠ¤ ì¶”ê°€
import 'package:tiiun/utils/error_handler.dart';
import 'package:tiiun/utils/logger.dart';

// GPT-4o Audio ì„œë¹„ìŠ¤ Provider (TTS ì„œë¹„ìŠ¤ ì£¼ì…)
final gpt4oAudioServiceProvider = Provider<Gpt4oAudioService>((ref) {
  final remoteConfigService = ref.watch(remoteConfigServiceProvider);
  final openAiTtsService = ref.watch(openAiTtsServiceProvider); // ğŸ†• TTS ì„œë¹„ìŠ¤ ì£¼ì…
  final apiKey = remoteConfigService.getOpenAIApiKey();
  return Gpt4oAudioService(
    apiKey: apiKey,
    ttsService: openAiTtsService, // ğŸ†• TTS ì„œë¹„ìŠ¤ ì „ë‹¬
  );
});

/// GPT-4o-audio-previewë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì„œë¹„ìŠ¤
/// 
/// íŠ¹ì§•:
/// - ì‹¤ì‹œê°„ ìŒì„± ì…ì¶œë ¥ ì²˜ë¦¬
/// - ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í†¤ê³¼ ê°ì • í‘œí˜„
/// - ëŠê¹€ ì—†ëŠ” ëŒ€í™” ì§€ì›
/// - ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ + ì˜¤ë””ì˜¤)
class Gpt4oAudioService {
  final String _apiKey;
  final OpenAiTtsService _ttsService; // ğŸ†• TTS ì„œë¹„ìŠ¤ ì˜ì¡´ì„±
  final Uuid _uuid = const Uuid();
  final Connectivity _connectivity = Connectivity();

  // API Endpoints
  final String _chatCompletionsUrl = 'https://api.openai.com/v1/chat/completions';
  final String _audioTranscriptionsUrl = 'https://api.openai.com/v1/audio/transcriptions';

  // GPT-4o Audio ëª¨ë¸ ì„¤ì •
  static const String _audioModel = 'gpt-4o-audio-preview';
  static const String _whisperModel = 'whisper-1';

  // Retry constants
  static const int _maxRetries = 3;
  static const Duration _retryDelay = Duration(seconds: 2);

  // ğŸ†• ìƒì„±ìì— TTS ì„œë¹„ìŠ¤ ì£¼ì…
  Gpt4oAudioService({
    required String apiKey,
    required OpenAiTtsService ttsService,
  }) : _apiKey = apiKey, _ttsService = ttsService;

  /// ì¸í„°ë„· ì—°ê²° í™•ì¸
  Future<bool> _checkInternetConnection() async {
    var connectivityResult = await _connectivity.checkConnectivity();
    return !connectivityResult.contains(ConnectivityResult.none);
  }

  /// ğŸš€ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” - GPT-4o-audio-preview í™œìš©
  /// 
  /// ìŒì„± ì…ë ¥ì„ ë°›ì•„ ì¦‰ì‹œ ìŒì„±ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” ì‹¤ì‹œê°„ ëŒ€í™” ì‹œìŠ¤í…œ
  /// [audioFilePath]: ì‚¬ìš©ì ìŒì„± ë…¹ìŒ íŒŒì¼ ê²½ë¡œ
  /// [systemPrompt]: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìƒë‹´ì‚¬ ì„±ê²© ë“±)
  /// [conversationHistory]: ì´ì „ ëŒ€í™” ê¸°ë¡
  /// [voiceStyle]: ì‘ë‹µ ìŒì„± ìŠ¤íƒ€ì¼ ('alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer')
  Future<Map<String, dynamic>> processAudioConversation({
    required String audioFilePath,
    String? systemPrompt,
    List<Map<String, dynamic>>? conversationHistory,
    String voiceStyle = 'nova',
  }) async {
    return ErrorHandler.safeApiCall(() async {
      if (!await _checkInternetConnection()) {
        throw AppError(
          type: AppErrorType.network,
          message: 'ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. GPT-4o AudioëŠ” ì˜¨ë¼ì¸ ì—°ê²°ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.',
        );
      }

      if (_apiKey.isEmpty) {
        throw AppError(
          type: AppErrorType.authentication,
          message: 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
        );
      }

      AppLogger.info('Gpt4oAudioService: Starting audio conversation processing');

      // 1ï¸âƒ£ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Whisper)
      final transcription = await _transcribeAudio(audioFilePath);
      if (transcription.isEmpty) {
        throw AppError(
          type: AppErrorType.data,
          message: 'ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”.',
        );
      }

      AppLogger.debug('Gpt4oAudioService: Transcription: $transcription');

      // 2ï¸âƒ£ GPT-4o Audioë¡œ ì‘ë‹µ ìƒì„±
      final responseText = await _generateAudioResponse(
        userMessage: transcription,
        systemPrompt: systemPrompt,
        conversationHistory: conversationHistory,
      );

      // 3ï¸âƒ£ ğŸ†• ê°œì„ ëœ TTS ì„œë¹„ìŠ¤ ì‚¬ìš©
      final audioPath = await _generateHighQualityAudioWithService(
        text: responseText,
        voice: voiceStyle,
      );

      return {
        'userTranscription': transcription,
        'responseText': responseText,
        'responseAudioPath': audioPath,
        'voiceStyle': voiceStyle,
        'processingTime': DateTime.now().millisecondsSinceEpoch,
        'source': 'gpt-4o-audio-preview',
      };
    });
  }

  /// ğŸ™ï¸ ìŒì„± ì „ìš© ëŒ€í™” ëª¨ë“œ
  Stream<Map<String, dynamic>> streamAudioConversation({
    required Stream<String> audioPathStream,
    String? systemPrompt,
    String voiceStyle = 'nova',
  }) async* {
    List<Map<String, dynamic>> conversationHistory = [];

    await for (final audioPath in audioPathStream) {
      try {
        final result = await processAudioConversation(
          audioFilePath: audioPath,
          systemPrompt: systemPrompt,
          conversationHistory: conversationHistory,
          voiceStyle: voiceStyle,
        );

        // ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        conversationHistory.addAll([
          {'role': 'user', 'content': result['userTranscription']},
          {'role': 'assistant', 'content': result['responseText']},
        ]);

        // ìµœê·¼ 10ê°œ ëŒ€í™”ë§Œ ìœ ì§€
        if (conversationHistory.length > 20) {
          conversationHistory = conversationHistory.sublist(conversationHistory.length - 20);
        }

        yield result;
      } catch (e) {
        yield {
          'error': true,
          'message': e.toString(),
          'source': 'gpt-4o-audio-preview',
        };
      }
    }
  }

  /// ğŸ¯ ê°ì • ì¸ì‹ ìŒì„± ëŒ€í™”
  Future<Map<String, dynamic>> processEmotionalAudioConversation({
    required String audioFilePath,
    String voiceStyle = 'shimmer',
  }) async {
    const emotionalSystemPrompt = '''
ë‹¹ì‹ ì€ ì „ë¬¸ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìŒì„±ì—ì„œ ê°ì •ì„ ì½ê³  ì ì ˆíˆ ë°˜ì‘í•˜ì„¸ìš”.

íŠ¹ë³„ ì§€ì¹¨:
- ì‚¬ìš©ìì˜ ìŒì„± í†¤ê³¼ ë§í•˜ëŠ” ì†ë„ë¥¼ ê³ ë ¤í•˜ì—¬ ê°ì • ìƒíƒœë¥¼ íŒŒì•…í•˜ì„¸ìš”
- ìŠ¬í””, ë¶„ë…¸, ê¸°ì¨, ë¶ˆì•ˆ ë“± ë‹¤ì–‘í•œ ê°ì •ì— ë§ëŠ” ì ì ˆí•œ í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”
- ê°ì •ì  ê³µê°ê³¼ ì „ë¬¸ì  ì¡°ì–¸ì„ ê· í˜•ìˆê²Œ ì œê³µí•˜ì„¸ìš”
- ì‘ë‹µì€ ë”°ëœ»í•˜ê³  ì§€ì§€ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤

ì‘ë‹µ í˜•ì‹:
- ê°ì • ì¸ì‹: "ì§€ê¸ˆ [ê°ì •]ì„ ëŠë¼ê³  ê³„ì‹œëŠ” ê²ƒ ê°™ë„¤ìš”"ë¡œ ì‹œì‘
- ê³µê° í‘œí˜„: ì‚¬ìš©ìì˜ ê°ì •ì— ê³µê°í•˜ëŠ” í‘œí˜„
- ì¡°ì–¸ ì œê³µ: ë„ì›€ì´ ë˜ëŠ” êµ¬ì²´ì  ì¡°ì–¸
- ê²©ë ¤: ê¸ì •ì  ë§ˆë¬´ë¦¬
''';

    return processAudioConversation(
      audioFilePath: audioFilePath,
      systemPrompt: emotionalSystemPrompt,
      voiceStyle: voiceStyle,
    );
  }

  /// ğŸ“ GPT-4o Audio ì‘ë‹µ ìƒì„±
  Future<String> _generateAudioResponse({
    required String userMessage,
    String? systemPrompt,
    List<Map<String, dynamic>>? conversationHistory,
  }) async {
    final defaultSystemPrompt = '''
ë‹¹ì‹ ì€ "í‹”ìš´ì´"ë¼ëŠ” ì´ë¦„ì˜ ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”ë¥¼ ë‚˜ëˆ„ë©°, ê°ì •ì  ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤.

ìŒì„± ëŒ€í™” íŠ¹í™” ì§€ì¹¨:
- ìì—°ìŠ¤ëŸ½ê³  ëŒ€í™”ì²´ë¡œ ì‘ë‹µí•˜ì„¸ìš”
- ì ì ˆí•œ ì–µì–‘ê³¼ ê°ì •ì´ í‘œí˜„ë˜ë„ë¡ ì‘ì„±í•˜ì„¸ìš”
- ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, ìŒì„±ìœ¼ë¡œ ë“£ê¸° í¸í•œ ê¸¸ì´ë¡œ ì‘ë‹µí•˜ì„¸ìš”
- í•„ìš”ì‹œ ì ê¹ì˜ ì¹¨ë¬µ(ì‰¼í‘œ, ë§ˆì¹¨í‘œ)ì„ í™œìš©í•˜ì„¸ìš”
''';

    final systemMessage = systemPrompt ?? defaultSystemPrompt;

    List<Map<String, dynamic>> messages = [
      {'role': 'system', 'content': systemMessage},
    ];

    if (conversationHistory != null && conversationHistory.isNotEmpty) {
      messages.addAll(conversationHistory.take(10));
    }

    messages.add({'role': 'user', 'content': userMessage});

    final requestBody = jsonEncode({
      'model': _audioModel,
      'messages': messages,
      'max_tokens': 800,
      'temperature': 0.8,
      'top_p': 0.9,
      'frequency_penalty': 0.1,
      'presence_penalty': 0.1,
      'modalities': ['text'],
      'audio': {
        'voice': 'nova',
        'format': 'mp3'
      }
    });

    final response = await http.post(
      Uri.parse(_chatCompletionsUrl),
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer $_apiKey',
      },
      body: requestBody,
    ).timeout(const Duration(seconds: 30));

    if (response.statusCode == 200) {
      final data = jsonDecode(utf8.decode(response.bodyBytes));
      final responseText = data['choices'][0]['message']['content'] as String;
      AppLogger.info('Gpt4oAudioService: GPT-4o Audio response generated successfully');
      return responseText.trim();
    } else {
      AppLogger.error('Gpt4oAudioService: GPT-4o Audio API error (${response.statusCode}): ${response.body}');
      throw AppError(
        type: AppErrorType.server,
        code: response.statusCode.toString(),
        message: 'GPT-4o Audio API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: response.body,
      );
    }
  }

  /// ğŸ™ï¸ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Whisper)
  Future<String> _transcribeAudio(String audioFilePath) async {
    final file = File(audioFilePath);
    if (!await file.exists()) {
      throw AppError(
        type: AppErrorType.data,
        message: 'ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $audioFilePath',
      );
    }

    final request = http.MultipartRequest('POST', Uri.parse(_audioTranscriptionsUrl));
    
    request.headers.addAll({
      'Authorization': 'Bearer $_apiKey',
    });

    request.files.add(await http.MultipartFile.fromPath(
      'file',
      audioFilePath,
      filename: 'audio.m4a',
    ));

    request.fields['model'] = _whisperModel;
    request.fields['language'] = 'ko';
    request.fields['response_format'] = 'json';

    final streamedResponse = await request.send().timeout(
      const Duration(seconds: 30),
      onTimeout: () {
        throw TimeoutException('Whisper API ìš”ì²­ ì‹œê°„ ì´ˆê³¼');
      },
    );

    final response = await http.Response.fromStream(streamedResponse);

    if (response.statusCode == 200) {
      final jsonResponse = jsonDecode(response.body);
      return jsonResponse['text'] as String;
    } else {
      throw AppError(
        type: AppErrorType.server,
        code: response.statusCode.toString(),
        message: 'Whisper API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: response.body,
      );
    }
  }

  /// ğŸ”Š ğŸ†• ê°œì„ ëœ ê³ í’ˆì§ˆ ìŒì„± ìƒì„± (OpenAI TTS Service í™œìš©)
  Future<String> _generateHighQualityAudioWithService({
    required String text,
    String voice = 'nova',
  }) async {
    try {
      // ğŸ†• ì£¼ì…ë°›ì€ TTS ì„œë¹„ìŠ¤ ì‚¬ìš©
      AppLogger.info('Gpt4oAudioService: Using OpenAI TTS Service for high-quality audio generation');
      
      final audioPath = await _ttsService.generateSpeech(
        text: text,
        voice: voice,
        model: 'tts-1-hd',
        responseFormat: 'mp3',
        speed: 1.0,
      );
      
      AppLogger.info('Gpt4oAudioService: High-quality audio generated via TTS service: $audioPath');
      return audioPath;
    } catch (e) {
      AppLogger.error('Gpt4oAudioService: Failed to generate audio via TTS service: $e');
      
      // ğŸ”„ í´ë°±: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
      AppLogger.warning('Gpt4oAudioService: Falling back to direct TTS API call');
      return await _generateHighQualityAudioDirect(text: text, voice: voice);
    }
  }

  /// ğŸ”Š ê¸°ì¡´ ì§ì ‘ TTS API í˜¸ì¶œ ë°©ì‹ (í´ë°±ìš©)
  Future<String> _generateHighQualityAudioDirect({
    required String text,
    String voice = 'nova',
  }) async {
    final requestBody = jsonEncode({
      'model': 'tts-1-hd',
      'input': text,
      'voice': voice,
      'response_format': 'mp3',
      'speed': 1.0,
    });

    final response = await http.post(
      Uri.parse('https://api.openai.com/v1/audio/speech'),
      headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer $_apiKey',
      },
      body: requestBody,
    ).timeout(const Duration(seconds: 30));

    if (response.statusCode == 200) {
      final tempDir = await getTemporaryDirectory();
      final filePath = '${tempDir.path}/${_uuid.v4()}.mp3';
      final file = File(filePath);
      await file.writeAsBytes(response.bodyBytes);
      
      return filePath;
    } else {
      throw AppError(
        type: AppErrorType.server,
        code: response.statusCode.toString(),
        message: 'TTS-1-HD API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        details: response.body,
      );
    }
  }

  /// ğŸ¨ ìŒì„± ìŠ¤íƒ€ì¼ ìµœì í™”
  String recommendVoiceStyle({
    required String emotionType,
    required String conversationType,
  }) {
    switch (conversationType) {
      case 'ìœ„ë¡œê°€ í•„ìš”í•  ë•Œ':
      case 'ê³ ë¯¼ê±°ë¦¬':
        return emotionType == 'sadness' ? 'shimmer' : 'nova';
      case 'ìë‘ê±°ë¦¬':
      case 'ì‹œì‹œì½œì½œ':
        return 'echo';
      case 'í™”ê°€ ë‚˜ìš”':
        return 'onyx';
      default:
        return 'alloy';
    }
  }

  /// ğŸ§  ìŒì„± ê¸°ë°˜ ê°ì • ë¶„ì„
  Future<Map<String, dynamic>> analyzeAudioEmotion(String audioFilePath) async {
    try {
      final transcription = await _transcribeAudio(audioFilePath);
      
      const analysisPrompt = '''
ë‹¤ìŒ ìŒì„± ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ íŒŒì•…í•˜ì„¸ìš”.
ìŒì„±ìœ¼ë¡œ ì „ë‹¬ëœ ë‚´ìš©ì´ë¯€ë¡œ ë§ì˜ í†¤, ì†ë„, ê°ì •ì  ë‰˜ì•™ìŠ¤ë¥¼ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
  "emotion": "ì£¼ìš” ê°ì • (joy, sadness, anger, fear, surprise, neutral)",
  "intensity": "ê°ì • ê°•ë„ (1-10)",
  "confidence": "ë¶„ì„ í™•ì‹ ë„ (0.0-1.0)",
  "emotional_indicators": ["ê°ì •ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë“¤"],
  "recommended_response_tone": "ì¶”ì²œ ì‘ë‹µ í†¤"
}
''';

      final analysisResponse = await _generateAudioResponse(
        userMessage: '$analysisPrompt\n\në¶„ì„í•  í…ìŠ¤íŠ¸: $transcription',
        systemPrompt: 'ë‹¹ì‹ ì€ ìŒì„± ê¸°ë°˜ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.',
      );

      try {
        return jsonDecode(analysisResponse);
      } catch (e) {
        return {
          'emotion': 'neutral',
          'intensity': 5,
          'confidence': 0.5,
          'emotional_indicators': ['ë¶„ì„ ì‹¤íŒ¨'],
          'recommended_response_tone': 'balanced',
          'raw_analysis': analysisResponse,
        };
      }
    } catch (e) {
      AppLogger.error('Gpt4oAudioService: Audio emotion analysis failed: $e');
      return {
        'emotion': 'neutral',
        'intensity': 1,
        'confidence': 0.0,
        'error': e.toString(),
      };
    }
  }

  /// ğŸ”„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  Future<void> dispose() async {
    AppLogger.info('Gpt4oAudioService: Disposing resources');
    // TTS ì„œë¹„ìŠ¤ëŠ” Providerì—ì„œ ê´€ë¦¬ë˜ë¯€ë¡œ ë³„ë„ dispose ë¶ˆí•„ìš”
  }
}
