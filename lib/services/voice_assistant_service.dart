import 'package:flutter_riverpod/flutter_riverpod.dart';
import 'package:langchain/langchain.dart';
import 'package:langchain_openai/langchain_openai.dart';
import 'package:flutter_tts/flutter_tts.dart';
import 'package:path_provider/path_provider.dart';
import 'dart:async';
import 'package:uuid/uuid.dart';
import 'package:flutter/foundation.dart';
import 'langchain_service.dart';
import 'conversation_memory_service.dart';
import 'voice_service.dart';
import 'whisper_service.dart';
import 'remote_config_service.dart'; // Remote Config ì„œë¹„ìŠ¤ ì¶”ê°€
import '../utils/simple_speech_recognizer.dart';
import 'package:connectivity_plus/connectivity_plus.dart';
import 'package:shared_preferences/shared_preferences.dart';

// ìŒì„± ì¸ì‹ ëª¨ë“œ ì—´ê±°í˜•
enum SpeechRecognitionMode {
  whisper,   // OpenAI Whisper API ì‚¬ìš©
  native,    // ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ì‚¬ìš©
}

// ìŒì„± ë¹„ì„œ ì„œë¹„ìŠ¤ Provider
final voiceAssistantServiceProvider = Provider<VoiceAssistantService>((ref) {
  try {
    final langchainService = ref.watch(langchainServiceProvider);
    final conversationMemoryService = ref.watch(conversationMemoryServiceProvider);
    final voiceService = ref.watch(voiceServiceProvider);
    final remoteConfigService = ref.watch(remoteConfigServiceProvider); // Remote Config ì¶”ê°€
    
    final service = VoiceAssistantService(langchainService, conversationMemoryService, voiceService);
    
    // API í‚¤ ìë™ ì„¤ì •
    final apiKey = remoteConfigService.getOpenAIApiKey();
    if (apiKey.isNotEmpty) {
      service.setApiKey(apiKey);
      debugPrint('VoiceAssistantService: API í‚¤ê°€ ìë™ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤');
    } else {
      debugPrint('VoiceAssistantService: API í‚¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Remote Configë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
    }
    
    return service;
  } catch (e) {
    // ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë¹ˆ ì„œë¹„ìŠ¤ ë°˜í™˜
    print('ìŒì„± ë¹„ì„œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: $e');
    return VoiceAssistantService.empty();
  }
});

// Whisper Service Provider
final whisperServiceProvider = Provider<WhisperService?>((ref) => null); // ì‹¤ì œ ì´ˆê¸°í™”ëŠ” setApiKeyì—ì„œ ìˆ˜í–‰

class VoiceAssistantService {
  // ë¹ˆ ì„œë¹„ìŠ¤ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ìƒì„±ì
  factory VoiceAssistantService.empty() {
    return VoiceAssistantService._empty();
  }
  
  VoiceAssistantService._empty() : 
    _langchainService = null, 
    _memoryService = null,
    _voiceService = null;

  // ì¼ë°˜ ìƒì„±ì
  VoiceAssistantService(
    this._langchainService, 
    this._memoryService,
    this._voiceService,
  );
  
  final LangchainService? _langchainService;
  final ConversationMemoryService? _memoryService;
  final VoiceService? _voiceService;
  
  bool _isListening = false;
  bool _isProcessing = false;
  
  // ìŒì„± ì¸ì‹ ê´€ë ¨ ë³€ìˆ˜
  final SimpleSpeechRecognizer _speechRecognizer = SimpleSpeechRecognizer();
  WhisperService? _whisperService;
  SpeechRecognitionMode _recognitionMode = SpeechRecognitionMode.whisper; // ê¸°ë³¸ê°’ Whisper
  
  // Text to Speech
  final FlutterTts _flutterTts = FlutterTts();
  
  // LLM Chain
  String? _apiKey;
  ConversationChain? _conversationChain;
  final Uuid _uuid = const Uuid();
  
  // ìƒíƒœ
  String _currentConversationId = '';
  StreamController<String>? _transcriptionStreamController;
  StreamController<Map<String, dynamic>>? _responseStreamController;
  StreamSubscription? _recognizerSubscription;
  StreamSubscription? _whisperStreamSubscription;
  
  // ì—°ê²° í™•ì¸
  final Connectivity _connectivity = Connectivity();
  
  // API í‚¤ ì„¤ì •
  void setApiKey(String apiKey) async {
    _apiKey = apiKey;
    _initConversationChain();
    
    // API í‚¤ê°€ ìˆì„ ë•Œë§Œ Whisper ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    if (apiKey.isNotEmpty) {
      try {
        _whisperService = WhisperService(apiKey: apiKey);
        debugPrint('Whisper ì„œë¹„ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤');
      } catch (e) {
        debugPrint('Whisper ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: $e - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤');
        _whisperService = null;
        _recognitionMode = SpeechRecognitionMode.native;
      }
    } else {
      debugPrint('API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤ - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤');
      _whisperService = null;
      _recognitionMode = SpeechRecognitionMode.native;
    }
    
    // ì„¤ì • ë³µì›
    await _loadSettings();
  }
  
  // ì„¤ì • ë¡œë“œ
  Future<void> _loadSettings() async {
    try {
      final prefs = await SharedPreferences.getInstance();
      final useWhisper = prefs.getBool('use_whisper_api') ?? true;
      _recognitionMode = useWhisper 
          ? SpeechRecognitionMode.whisper 
          : SpeechRecognitionMode.native;
    } catch (e) {
      debugPrint('ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: $e');
    }
  }
  
  // ì„¤ì • ì €ì¥
  Future<void> _saveSettings() async {
    try {
      final prefs = await SharedPreferences.getInstance();
      await prefs.setBool(
        'use_whisper_api', 
        _recognitionMode == SpeechRecognitionMode.whisper
      );
    } catch (e) {
      debugPrint('ì„¤ì • ì €ì¥ ì‹¤íŒ¨: $e');
    }
  }
  
  // ê¸°ê¸° ì„¤ì • ì´ˆê¸°í™”
  Future<void> initSpeech() async {
    try {
      // ìŒì„± ì¸ì‹ ì´ˆê¸°í™” (backupìœ¼ë¡œ ìœ ì§€)
      await _speechRecognizer.initialize();
      
      // TTS ì„¤ì • ì´ˆê¸°í™”
      await _flutterTts.setLanguage('ko-KR');
      await _flutterTts.setSpeechRate(0.5);
      await _flutterTts.setVolume(1.0);
      await _flutterTts.setPitch(1.0);
    } catch (e) {
      print('ìŒì„± ì´ˆê¸°í™” ì‹¤íŒ¨: $e');
    }
  }
  
  // ìŒì„± ì¸ì‹ ëª¨ë“œ ì„¤ì •
  Future<void> setRecognitionMode(SpeechRecognitionMode mode) async {
    _recognitionMode = mode;
    await _saveSettings();
  }
  
  // OpenAI Whisper ì‚¬ìš© ì—¬ë¶€ ì„¤ì •
  void setUseWhisper(bool useWhisper) {
    _recognitionMode = useWhisper 
        ? SpeechRecognitionMode.whisper 
        : SpeechRecognitionMode.native;
    _saveSettings();
  }
  
  // ì¸í„°ë„· ì—°ê²° í™•ì¸
  Future<bool> _checkInternetConnection() async {
    var connectivityResult = await _connectivity.checkConnectivity();
    return connectivityResult != ConnectivityResult.none;
  }
  
  // ìŒì„± ì¸ì‹ ì‹œì‘
  Stream<String> startListening() {
    _transcriptionStreamController = StreamController<String>();
    
    if (_isListening) {
      _transcriptionStreamController?.add('[error]ì´ë¯¸ ìŒì„± ì¸ì‹ ì¤‘ì…ë‹ˆë‹¤');
      return _transcriptionStreamController!.stream;
    }
    
    if (_isProcessing) {
      _transcriptionStreamController?.add('[error]í˜„ì¬ ì‘ë‹µì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤');
      return _transcriptionStreamController!.stream;
    }
    
    _isListening = true;
    
    try {
      // API í‚¤ê°€ ì—†ê±°ë‚˜ Whisper ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ìë™ìœ¼ë¡œ ê¸°ê¸° ë‚´ì¥ ëª¨ë“œë¡œ ì „í™˜
      if (_recognitionMode == SpeechRecognitionMode.whisper && (_whisperService == null || _apiKey == null || _apiKey!.isEmpty)) {
        debugPrint('Whisper ì¡°ê±´ì´ ë§ì§€ ì•ŠìŒ - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜');
        _recognitionMode = SpeechRecognitionMode.native;
        _transcriptionStreamController?.add('[error]Whisper ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.');
      }
      
      // í˜„ì¬ ì„¤ì •ëœ ìŒì„± ì¸ì‹ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ì¸ì‹ ì‹œì‘
      if (_recognitionMode == SpeechRecognitionMode.whisper) {
        _startWhisperRecognition();
      } else {
        _startNativeSpeechRecognition();
      }
      
      return _transcriptionStreamController!.stream;
    } catch (e) {
      _isListening = false;
      _transcriptionStreamController?.add('[error]ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨: $e');
      return _transcriptionStreamController!.stream;
    }
  }
  
  // Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ ì‹œì‘
  Future<void> _startWhisperRecognition() async {
    if (_whisperService == null) {
      _transcriptionStreamController?.add('[error]Whisper ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.');
      _isListening = false;
      
      // ìë™ìœ¼ë¡œ ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜
      debugPrint('Whisper ì´ˆê¸°í™” ì‹¤íŒ¨ - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜');
      _recognitionMode = SpeechRecognitionMode.native;
      _startNativeSpeechRecognition();
      return;
    }
    
    // ì¸í„°ë„· ì—°ê²° í™•ì¸
    if (!await _checkInternetConnection()) {
      _transcriptionStreamController?.add('[error]ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.');
      _isListening = false;
      
      // ìë™ìœ¼ë¡œ ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜
      _recognitionMode = SpeechRecognitionMode.native;
      _startNativeSpeechRecognition();
      return;
    }
    
    try {
      debugPrint("OpenAI Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹ ì‹œì‘");
      
      // ë…¹ìŒ ë° ë³€í™˜ ìŠ¤íŠ¸ë¦¼ ì‹œì‘
      final whisperStream = _whisperService!.streamRecordAndTranscribe(
        recordingDuration: 10, // 10ì´ˆê°„ ë…¹ìŒ
        language: 'ko'         // í•œêµ­ì–´
      );
      
      // ìŠ¤íŠ¸ë¦¼ êµ¬ë…
      _whisperStreamSubscription = whisperStream.listen(
        (result) {
          if (result.startsWith('[error]')) {
            // ì˜¤ë¥˜ ì²˜ë¦¬
            debugPrint("Whisper ì¸ì‹ ì˜¤ë¥˜: ${result.substring(7)}");
            _transcriptionStreamController?.add(result);
            
            // ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ê¸° ë‚´ì¥ ì¸ì‹ìœ¼ë¡œ ìë™ ì „í™˜
            if (result.contains('ì¸í„°ë„· ì—°ê²°') || result.contains('API ì˜¤ë¥˜') || result.contains('401') || result.contains('403')) {
              debugPrint('Whisper ì˜¤ë¥˜ ë°œìƒ - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜');
              _recognitionMode = SpeechRecognitionMode.native;
            }
          } else if (result.startsWith('[listening_stopped]')) {
            // ì¸ì‹ ì¢…ë£Œ ì²˜ë¦¬
            if (_isListening) {
              _isListening = false;
              _transcriptionStreamController?.add('[listening_stopped]');
            }
          } else if (result.startsWith('[interim]')) {
            // ì¤‘ê°„ ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
            _transcriptionStreamController?.add(result);
          } else {
            // ìµœì¢… ê²°ê³¼ ì „ë‹¬ ë° ì¸ì‹ ì¢…ë£Œ
            _transcriptionStreamController?.add(result);
            _isListening = false;
            _transcriptionStreamController?.add('[listening_stopped]');
          }
        },
        onError: (error) {
          debugPrint("Whisper ìŒì„± ì¸ì‹ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: $error");
          _transcriptionStreamController?.add('[error]ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.');
          
          // ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ê¸° ë‚´ì¥ ì¸ì‹ìœ¼ë¡œ ìë™ ì „í™˜
          debugPrint('Whisper ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜ - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜');
          _recognitionMode = SpeechRecognitionMode.native;
          _isListening = false;
          _transcriptionStreamController?.add('[listening_stopped]');
        },
        onDone: () {
          if (_isListening) {
            _isListening = false;
            _transcriptionStreamController?.add('[listening_stopped]');
          }
        }
      );
    } catch (e) {
      debugPrint("Whisper ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜: $e");
      _transcriptionStreamController?.add('[error]Whisper ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨. ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.');
      
      // ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ê¸° ë‚´ì¥ ì¸ì‹ìœ¼ë¡œ ìë™ ì „í™˜
      debugPrint('Whisper ì‹œì‘ ì˜ˆì™¸ - ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ìœ¼ë¡œ ì „í™˜');
      _recognitionMode = SpeechRecognitionMode.native;
      _isListening = false;
    }
  }
  
  // ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ì‹œì‘
  void _startNativeSpeechRecognition() {
    try {
      debugPrint("ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ì‹œì‘");
      
      // ìŒì„± ì¸ì‹ê¸° ì‹œì‘
      _speechRecognizer.startListening();
      
      // ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ êµ¬ë…
      _recognizerSubscription = _speechRecognizer.transcriptionStream.listen(
        (result) {
          if (result.startsWith('[error]')) {
            // ì˜¤ë¥˜ ì²˜ë¦¬
            debugPrint("ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${result.substring(7)}");
            _transcriptionStreamController?.add(result);
          } else if (result.startsWith('[listening_stopped]')) {
            // ì¸ì‹ ì¢…ë£Œ ì²˜ë¦¬
            if (_isListening) {
              _isListening = false;
              _transcriptionStreamController?.add('[listening_stopped]');
            }
          } else if (result.startsWith('[interim]')) {
            // ì¤‘ê°„ ê²°ê³¼ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
            _transcriptionStreamController?.add(result);
          } else {
            // ìµœì¢… ê²°ê³¼ ì „ë‹¬ ë° ì¸ì‹ ì¢…ë£Œ
            _transcriptionStreamController?.add(result);
            _isListening = false;
            _transcriptionStreamController?.add('[listening_stopped]');
          }
        },
        onError: (error) {
          debugPrint("ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜: $error");
          _transcriptionStreamController?.add('[error]ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
          _isListening = false;
        },
      );
    } catch (e) {
      debugPrint("ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ì‹œì‘ ì˜¤ë¥˜: $e");
      _transcriptionStreamController?.add('[error]ìŒì„± ì¸ì‹ ì‹œì‘ ì‹¤íŒ¨: $e');
      _isListening = false;
    }
  }
  
  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  Future<void> stopListening() async {
    if (!_isListening) {
      return;
    }
    
    _isListening = false;
    
    try {
      // Whisper ì‚¬ìš© ì¤‘ì´ì—ˆë‹¤ë©´ ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
      await _whisperStreamSubscription?.cancel();
      
      // ê¸°ê¸° ë‚´ì¥ ìŒì„± ì¸ì‹ ì¤‘ì§€
      await _speechRecognizer.stopListening();
      await _recognizerSubscription?.cancel();
      
      _transcriptionStreamController?.add('[listening_stopped]');
    } catch (e) {
      debugPrint('ìŒì„± ì¸ì‹ ì¤‘ì§€ ì˜¤ë¥˜: $e');
    }
  }
  
  // ëŒ€í™” ì‹œì‘ ë˜ëŠ” ê³„ì† - Future<void>ë¡œ ë³€ê²½
  Future<void> startConversation(String conversationId) async {
    _currentConversationId = conversationId.isNotEmpty
        ? conversationId
        : _uuid.v4();
  }
  
  // ìŒì„± ì‘ë‹µ ì²˜ë¦¬
  Stream<Map<String, dynamic>> processVoiceInput(
    String text,
    String voiceId,
  ) {
    _responseStreamController = StreamController<Map<String, dynamic>>();
    
    if (_isProcessing) {
      _responseStreamController?.add({
        'status': 'error',
        'message': 'ì´ë¯¸ ì‘ë‹µ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤',
      });
      _responseStreamController?.close();
      return _responseStreamController!.stream;
    }
    
    if (_langchainService == null) {
      _responseStreamController?.add({
        'status': 'error',
        'message': 'ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤',
      });
      _responseStreamController?.close();
      return _responseStreamController!.stream;
    }
    
    if (text.isEmpty) {
      _responseStreamController?.add({
        'status': 'error',
        'message': 'ìŒì„± ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤',
      });
      _responseStreamController?.close();
      return _responseStreamController!.stream;
    }
    
    _isProcessing = true;
    
    // ì‘ë‹µ ìƒì„± í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    _responseStreamController?.add({
      'status': 'processing',
      'message': 'ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘...',
    });
    
    _getAIResponse(text, voiceId).then((response) {
      _isProcessing = false;
      _responseStreamController?.add({
        'status': 'completed',
        'response': response,
      });
      _responseStreamController?.close();
    }).catchError((error) {
      _isProcessing = false;
      _responseStreamController?.add({
        'status': 'error',
        'message': 'ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: $error',
      });
      _responseStreamController?.close();
    });
    
    return _responseStreamController!.stream;
  }
  
  // AI ì‘ë‹µ ìƒì„±
  Future<Map<String, dynamic>> _getAIResponse(
    String userMessage,
    String voiceId,
  ) async {
    try {
      if (_langchainService == null) {
        throw Exception('ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }
      
      // LangChain ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì‘ë‹µ ìƒì„±
      final response = await _langchainService!.getResponse(
        conversationId: _currentConversationId,
        userMessage: userMessage,
      );
      
      // í…ìŠ¤íŠ¸ ì‘ë‹µ
      final textResponse = response.text;
      
      // TTSë¥¼ í†µí•œ ìŒì„± ì‘ë‹µ ìƒì„±
      final audioFilePath = await _generateTtsAudio(textResponse, voiceId);
      
      return {
        'text': textResponse,
        'audioPath': audioFilePath,
        'voiceId': voiceId,
      };
    } catch (e) {
      throw Exception('ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: $e');
    }
  }
  
  // TTSë¥¼ í†µí•œ ìŒì„± íŒŒì¼ ìƒì„±
  Future<String> _generateTtsAudio(String text, String voiceId) async {
    try {
      // ë¨¼ì € nullì´ ì•„ë‹ˆë¼ë©´ VoiceService ì‚¬ìš© ì‹œë„
      if (_voiceService != null) {
        debugPrint('VoiceAssistant: VoiceServiceë¥¼ ì‚¬ìš©í•˜ì—¬ TTS ìƒì„±');
        try {
          // VoiceService ì‚¬ìš©
          final result = await _voiceService!.textToSpeechFile(text, voiceId);
          final url = result['url'];
          if (url != null && url.isNotEmpty) {
            debugPrint('VoiceAssistant: ì„±ê³µì ìœ¼ë¡œ TTS íŒŒì¼ ìƒì„±: $url');
            return url;
          } else {
            debugPrint('VoiceAssistant: VoiceServiceì—ì„œ URL ë°˜í™˜ ì‹¤íŒ¨, ë‚´ë¶€ TTSë¡œ ëŒ€ì²´');
          }
        } catch (e) {
          debugPrint('VoiceAssistant: VoiceService TTS ì˜¤ë¥˜, ë‚´ë¶€ TTSë¡œ ëŒ€ì²´: $e');
        }
      }
      
      // VoiceServiceê°€ nullì´ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‚´ë¶€ TTS ì‚¬ìš©
      debugPrint('VoiceAssistant: ë‚´ë¶€ TTS ì‚¬ìš©');
      // ìŒì„± ì„¤ì •
      try {
        switch (voiceId) {
          case 'male_1':
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-local', 'locale': 'ko-KR'});
            break;
          case 'child_1':
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-child-local', 'locale': 'ko-KR'});
            break;
          case 'calm_1':
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-calm-local', 'locale': 'ko-KR'});
            break;
          case 'alloy':
          case 'echo':
          case 'fable':
          case 'onyx':
          case 'nova':
          case 'shimmer':
            // OpenAI ìŒì„± IDê°€ ì „ë‹¬ëœ ê²½ìš° ê¸°ë³¸ ìŒì„± ì‚¬ìš©
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-local', 'locale': 'ko-KR'});
            break;
          default:
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-local', 'locale': 'ko-KR'});
        }
        
        // ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ê²½ë¡œ
        final tempDir = await getTemporaryDirectory();
        final filePath = '${tempDir.path}/tts_${_uuid.v4()}.mp3';
        
        // ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„±
        debugPrint('VoiceAssistant: íŒŒì¼ë¡œ TTS ìƒì„± ì‹œì‘: $filePath');
        await _flutterTts.synthesizeToFile(text, filePath);
        debugPrint('VoiceAssistant: íŒŒì¼ë¡œ TTS ìƒì„± ì™„ë£Œ');
        
        return filePath;
      } catch (innerError) {
        debugPrint('VoiceAssistant: ë‚´ë¶€ TTS ìƒì„± ì˜¤ë¥˜: $innerError');
        throw Exception('ëª¨ë“  TTS ë°©ì‹ ì‹¤íŒ¨: $innerError');
      }
    } catch (e) {
      debugPrint('VoiceAssistant: TTS ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: $e');
      throw Exception('TTS ìƒì„± ì¤‘ ì˜¤ë¥˜: $e');
    }
  }
  
  // í…ìŠ¤íŠ¸ë¡œ ìŒì„± ì¬ìƒ
  Future<void> speak(String text, String voiceId) async {
    try {
      if (_voiceService != null) {
        // VoiceService ì‚¬ìš©
        await _voiceService!.speak(text);
      } else {
        // ìŒì„± ì„¤ì •
        switch (voiceId) {
          case 'male_1':
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-local', 'locale': 'ko-KR'});
            break;
          case 'child_1':
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-child-local', 'locale': 'ko-KR'});
            break;
          case 'calm_1':
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-calm-local', 'locale': 'ko-KR'});
            break;
          default:
            await _flutterTts.setVoice({'name': 'ko-kr-x-ism-local', 'locale': 'ko-KR'});
        }
        
        await _flutterTts.speak(text);
      }
    } catch (e) {
      debugPrint('TTS ì¬ìƒ ì˜¤ë¥˜: $e');
      throw Exception('TTS ì¬ìƒ ì¤‘ ì˜¤ë¥˜: $e');
    }
  }
  
  // ìŒì„± ì¬ìƒ ì¤‘ì§€
  Future<void> stopSpeaking() async {
    if (_voiceService != null) {
      await _voiceService!.stopSpeaking();
    } else {
      await _flutterTts.stop();
    }
  }
  
  // ëŒ€í™” ì¢…ë£Œ
  Future<void> endConversation() async {
    await stopListening();
    await stopSpeaking();
    _recognizerSubscription?.cancel();
    _whisperStreamSubscription?.cancel();
    _transcriptionStreamController?.close();
    _responseStreamController?.close();
  }
  
  // ë¦¬ì†ŒìŠ¤ í•´ì œ
  Future<void> dispose() async {
    await stopListening();
    await stopSpeaking();
    _recognizerSubscription?.cancel();
    _whisperStreamSubscription?.cancel();
    _transcriptionStreamController?.close();
    _responseStreamController?.close();
    await _flutterTts.stop();
    await _speechRecognizer.dispose();
    
    // Whisper ì„œë¹„ìŠ¤ ì •ë¦¬
    await _whisperService?.dispose();
  }
  
  // ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëŒ€í™”ë¥¼ ìœ„í•œ LangChain ì²´ì¸ ìƒì„±
  Future<void> _initConversationChain() async {
    if (_apiKey == null || _apiKey!.isEmpty) {
      return;
    }
    
    try {
      // 1. ì±„íŒ… ëª¨ë¸ ì´ˆê¸°í™”
      final llm = ChatOpenAI(
        apiKey: _apiKey,
        temperature: 0.7,
        maxTokens: 1200, // ğŸ”¥ INCREASED: 1000 -> 1200 for better responses
        model: 'gpt-4o', // ğŸš€ UPGRADED: gpt-3.5-turbo -> gpt-4o
      );
      
      // 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
      final promptTemplate = ChatPromptTemplate.fromPromptMessages([
        SystemChatMessagePromptTemplate.fromTemplate("""
ë‹¹ì‹ ì€ ì •ì„œì  ì§€ì›ê³¼ ê³µê°ì„ ì œê³µí•˜ëŠ” ìƒë‹´ AIì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì •ì— ê³µê°í•˜ê³ , ì‹¬ë¦¬ì  ì•ˆì •ê°ì„ ì£¼ëŠ” ëŒ€í™”ë¥¼ í•˜ì„¸ìš”.
ê¸ì •ì ì´ê³  ì§€ì§€ì ì¸ íƒœë„ë¡œ ì‚¬ìš©ìê°€ ìì‹ ì˜ ê°ì •ì„ í‘œí˜„í•˜ë„ë¡ ê²©ë ¤í•˜ì„¸ìš”.
ëŒ€í™”ëŠ” ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ê³ , ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ ì‘ë‹µí•˜ì„¸ìš”.

ëŒ€í™” ê¸°ë¡:
{chat_history}
"""),
        HumanChatMessagePromptTemplate.fromTemplate("{question}"),
      ]);
      
      // 3. ëŒ€í™” ë©”ëª¨ë¦¬ ì„¤ì •
      final memory = ConversationBufferMemory(
        returnMessages: true,
        inputKey: 'question',
        outputKey: 'answer',
        memoryKey: 'chat_history',
      );
      
      // 4. ëŒ€í™” ì²´ì¸ ìƒì„±
      _conversationChain = ConversationChain(
        llm: llm,
        prompt: promptTemplate,
        memory: memory,
        outputParser: const StringOutputParser(),
      );
    } catch (e) {
      debugPrint('ëŒ€í™” ì²´ì¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: $e');
    }
  }
  
  // ìŒì„± ì¸ì‹ ìƒíƒœ í™•ì¸
  bool get isListening => _isListening;
  
  // ì‘ë‹µ ì²˜ë¦¬ ìƒíƒœ í™•ì¸
  bool get isProcessing => _isProcessing;
  
  // Whisper ì‚¬ìš© ì—¬ë¶€ í™•ì¸
  bool get isUsingWhisper => _recognitionMode == SpeechRecognitionMode.whisper;
  
  // í˜„ì¬ ì¸ì‹ ëª¨ë“œ ê°€ì ¸ì˜¤ê¸°
  SpeechRecognitionMode get recognitionMode => _recognitionMode;
  
  // Whisper ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ìƒíƒœ í™•ì¸
  bool get isWhisperServiceReady => _whisperService != null;
  
  // API í‚¤ ì„¤ì • ìƒíƒœ í™•ì¸
  bool get isApiKeySet => _apiKey != null && _apiKey!.isNotEmpty;
  
  // ìŒì„± ì¸ì‹ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
  bool get isSpeechRecognitionAvailable {
    return _recognitionMode == SpeechRecognitionMode.native || 
           (_recognitionMode == SpeechRecognitionMode.whisper && isWhisperServiceReady && isApiKeySet);
  }
}
