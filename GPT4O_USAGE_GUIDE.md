# ğŸš€ GPT-4o & Audio ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨
1. [ê¸°ë³¸ ì‚¬ìš©ë²•](#ê¸°ë³¸-ì‚¬ìš©ë²•)
2. [GPT-4o-audio-preview í™œìš©](#gpt-4o-audio-preview-í™œìš©)
3. [ì„±ëŠ¥ ìµœì í™” íŒ](#ì„±ëŠ¥-ìµœì í™”-íŒ)
4. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
5. [ëª¨ë²” ì‚¬ë¡€](#ëª¨ë²”-ì‚¬ë¡€)

## ğŸ¯ ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ê¸°ì¡´ ì½”ë“œ ë³€ê²½ ì—†ì´ ìë™ ì—…ê·¸ë ˆì´ë“œ

ê¸°ì¡´ì˜ ëª¨ë“  AI ì„œë¹„ìŠ¤ë“¤ì´ ìë™ìœ¼ë¡œ GPT-4oë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```dart
// ë³€ê²½ ì—†ìŒ - ìë™ìœ¼ë¡œ GPT-4o ì‚¬ìš©
final langchainService = ref.watch(langchainServiceProvider);
final response = await langchainService.getResponse(
  conversationId: conversationId,
  userMessage: userMessage,
);
```

### 2. ê³ í’ˆì§ˆ TTS ì‚¬ìš©

TTS ì„œë¹„ìŠ¤ë„ ìë™ìœ¼ë¡œ TTS-1-HDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```dart
// ë³€ê²½ ì—†ìŒ - ìë™ìœ¼ë¡œ TTS-1-HD ì‚¬ìš©
final voiceService = ref.watch(voiceServiceProvider);
final result = await voiceService.textToSpeechFile(text, voiceId);
```

## ğŸ™ï¸ GPT-4o-audio-preview í™œìš©

### 1. ê¸°ë³¸ ì„¤ì •

ìƒˆë¡œìš´ GPT-4o Audio ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´:

```dart
import 'package:tiiun/services/gpt4o_audio_service.dart';

class AudioChatPage extends ConsumerWidget {
  @override
  Widget build(BuildContext context, WidgetRef ref) {
    final gpt4oAudio = ref.watch(gpt4oAudioServiceProvider);
    
    return Scaffold(
      // UI êµ¬ì„±
    );
  }
}
```

### 2. ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”

```dart
Future<void> handleVoiceConversation(String audioFilePath) async {
  try {
    final result = await gpt4oAudio.processAudioConversation(
      audioFilePath: audioFilePath,
      systemPrompt: '''
      ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
      ì‚¬ìš©ìì˜ ê°ì •ì— ê³µê°í•˜ë©° ë„ì›€ì´ ë˜ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.
      ''',
      voiceStyle: 'nova', // ì¹œê·¼í•œ ì—¬ì„± ëª©ì†Œë¦¬
    );
    
    // ê²°ê³¼ í™œìš©
    final userSaid = result['userTranscription'];
    final aiResponse = result['responseText'];
    final audioPath = result['responseAudioPath'];
    
    // UI ì—…ë°ì´íŠ¸
    setState(() {
      messages.add(Message(sender: 'user', content: userSaid));
      messages.add(Message(sender: 'ai', content: aiResponse));
    });
    
    // ìŒì„± ì¬ìƒ
    await audioPlayer.setFilePath(audioPath);
    await audioPlayer.play();
    
  } catch (e) {
    print('ìŒì„± ëŒ€í™” ì˜¤ë¥˜: $e');
  }
}
```

### 3. ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ ì‹¤ì‹œê°„ ëŒ€í™”

```dart
Stream<String> audioPathStream = ...; // ìŒì„± íŒŒì¼ ìŠ¤íŠ¸ë¦¼

await for (final result in gpt4oAudio.streamAudioConversation(
  audioPathStream: audioPathStream,
  voiceStyle: 'shimmer', // ì°¨ë¶„í•œ ëª©ì†Œë¦¬
)) {
  if (result['error'] == true) {
    print('ì˜¤ë¥˜: ${result['message']}');
    continue;
  }
  
  // ì‹¤ì‹œê°„ ëŒ€í™” ì²˜ë¦¬
  final transcription = result['userTranscription'];
  final response = result['responseText'];
  final audioPath = result['responseAudioPath'];
  
  // UI ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
  updateChatUI(transcription, response);
  playAudio(audioPath);
}
```

### 4. ê°ì • ê¸°ë°˜ ìƒë‹´

```dart
Future<void> emotionalCounseling(String audioPath) async {
  // ê°ì • ë¶„ì„
  final emotionData = await gpt4oAudio.analyzeAudioEmotion(audioPath);
  
  final emotion = emotionData['emotion']; // 'sadness', 'joy', etc.
  final intensity = emotionData['intensity']; // 1-10
  
  // ê°ì •ì— ë§ëŠ” ì‘ë‹µ
  final result = await gpt4oAudio.processEmotionalAudioConversation(
    audioFilePath: audioPath,
    voiceStyle: gpt4oAudio.recommendVoiceStyle(
      emotionType: emotion,
      conversationType: 'ìœ„ë¡œê°€ í•„ìš”í•  ë•Œ',
    ),
  );
  
  // ê°ì • ê¸°ë°˜ UI ì—…ë°ì´íŠ¸
  updateEmotionalIndicator(emotion, intensity);
  playResponseAudio(result['responseAudioPath']);
}
```

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### 1. í† í° ì‚¬ìš© ìµœì í™”

```dart
// ğŸš« ë¹„íš¨ìœ¨ì  - ë„ˆë¬´ ê¸´ í”„ë¡¬í”„íŠ¸
final response = await langchainService.getResponse(
  userMessage: '''
  ë§¤ìš° ê¸´ ì‚¬ìš©ì ë©”ì‹œì§€...
  ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ë“¤...
  ë°˜ë³µì ì¸ ë‚´ìš©ë“¤...
  ''',
);

// âœ… íš¨ìœ¨ì  - í•µì‹¬ë§Œ ì „ë‹¬
final response = await langchainService.getResponse(
  userMessage: 'ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šì•„ìš”. ìœ„ë¡œí•´ì£¼ì„¸ìš”.',
);
```

### 2. ìºì‹± í™œìš©

```dart
class ChatService {
  final Map<String, String> _responseCache = {};
  
  Future<String> getCachedResponse(String input) async {
    // ìºì‹œ í™•ì¸
    if (_responseCache.containsKey(input)) {
      return _responseCache[input]!;
    }
    
    // GPT-4o í˜¸ì¶œ
    final response = await langchainService.getResponse(
      userMessage: input,
    );
    
    // ìºì‹œ ì €ì¥
    _responseCache[input] = response;
    return response;
  }
}
```

### 3. ë³‘ë ¬ ì²˜ë¦¬

```dart
Future<Map<String, dynamic>> processAudioInParallel(String audioPath) async {
  // ë™ì‹œì— ì²˜ë¦¬
  final futures = await Future.wait([
    gpt4oAudio.analyzeAudioEmotion(audioPath), // ê°ì • ë¶„ì„
    whisperService.transcribeAudio(audioPath), // ìŒì„± ì¸ì‹
  ]);
  
  final emotionData = futures[0] as Map<String, dynamic>;
  final transcription = futures[1] as String;
  
  return {
    'emotion': emotionData,
    'transcription': transcription,
  };
}
```

### 4. ìŒì„± í’ˆì§ˆë³„ ëª¨ë¸ ì„ íƒ

```dart
class AdaptiveTtsService {
  Future<String> generateSpeech(String text, {bool highQuality = true}) async {
    if (highQuality && text.length > 100) {
      // ê¸´ í…ìŠ¤íŠ¸ + ê³ í’ˆì§ˆ = TTS-1-HD
      return openAiTtsService.generateSpeech(
        text: text,
        model: 'tts-1-hd',
      );
    } else {
      // ì§§ì€ í…ìŠ¤íŠ¸ = ì¼ë°˜ TTS (ë¹„ìš© ì ˆì•½)
      return openAiTtsService.generateSpeech(
        text: text,
        model: 'tts-1',
      );
    }
  }
}
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### 1. API ì‘ë‹µ ì†ë„ ì €í•˜

```dart
// ë¬¸ì œ: ì‘ë‹µì´ ë„ˆë¬´ ëŠë¦¼
// í•´ê²°ì±…: íƒ€ì„ì•„ì›ƒê³¼ ì¬ì‹œë„ êµ¬í˜„

Future<String> robustApiCall(String message) async {
  int retryCount = 0;
  const maxRetries = 3;
  
  while (retryCount < maxRetries) {
    try {
      final response = await langchainService.getResponse(
        userMessage: message,
      ).timeout(
        Duration(seconds: 30), // 30ì´ˆ íƒ€ì„ì•„ì›ƒ
      );
      
      return response;
    } catch (e) {
      retryCount++;
      if (retryCount >= maxRetries) rethrow;
      
      // ì¬ì‹œë„ ì „ ëŒ€ê¸°
      await Future.delayed(Duration(seconds: 2));
    }
  }
  
  throw Exception('ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼');
}
```

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê´€ë¦¬

```dart
class ConversationManager {
  List<Message> _messages = [];
  static const int maxMessages = 20; // ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜
  
  void addMessage(Message message) {
    _messages.add(message);
    
    // ë©”ëª¨ë¦¬ ê´€ë¦¬: ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±°
    if (_messages.length > maxMessages) {
      _messages.removeRange(0, _messages.length - maxMessages);
    }
  }
  
  // ì£¼ê¸°ì  ì •ë¦¬
  void cleanup() {
    // ì˜¤ë˜ëœ ì˜¤ë””ì˜¤ íŒŒì¼ ì •ë¦¬
    cleanupOldAudioFiles();
    
    // ìºì‹œ ì •ë¦¬
    clearResponseCache();
  }
}
```

### 3. ì˜¤í”„ë¼ì¸ ì§€ì›

```dart
class HybridAiService {
  Future<String> getResponse(String message) async {
    try {
      // ì˜¨ë¼ì¸: GPT-4o ì‚¬ìš©
      if (await hasInternetConnection()) {
        return await langchainService.getResponse(userMessage: message);
      }
    } catch (e) {
      print('ì˜¨ë¼ì¸ AI ì‹¤íŒ¨: $e');
    }
    
    // ì˜¤í”„ë¼ì¸: ë¡œì»¬ ì‘ë‹µ
    return generateOfflineResponse(message);
  }
  
  String generateOfflineResponse(String message) {
    // ê·œì¹™ ê¸°ë°˜ ì‘ë‹µ ë˜ëŠ” ìºì‹œëœ ì‘ë‹µ
    if (message.contains('ì•ˆë…•')) {
      return 'ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ì˜¤í”„ë¼ì¸ ëª¨ë“œì…ë‹ˆë‹¤.';
    }
    return 'ì£„ì†¡í•©ë‹ˆë‹¤. ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.';
  }
}
```

## ğŸ† ëª¨ë²” ì‚¬ë¡€

### 1. ì‚¬ìš©ì ê²½í—˜ ìš°ì„ 

```dart
class ChatViewModel extends ChangeNotifier {
  bool _isTyping = false;
  bool get isTyping => _isTyping;
  
  Future<void> sendMessage(String message) async {
    // ì¦‰ì‹œ UI ë°˜ì˜
    addUserMessage(message);
    
    // íƒ€ì´í•‘ í‘œì‹œ
    _isTyping = true;
    notifyListeners();
    
    try {
      final response = await langchainService.getResponse(
        userMessage: message,
      );
      
      addAiMessage(response);
    } catch (e) {
      addErrorMessage('ì‘ë‹µì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    } finally {
      _isTyping = false;
      notifyListeners();
    }
  }
}
```

### 2. ì ì§„ì  ê¸°ëŠ¥ ì œê³µ

```dart
class FeatureFlags {
  static const bool useGpt4oAudio = true;
  static const bool useHighQualityTts = true;
  
  static bool shouldUseAdvancedFeatures(User user) {
    return user.isPremium || user.isBetaTester;
  }
}

class AdaptiveAiService {
  Future<String> getResponse(String message, User user) async {
    if (FeatureFlags.shouldUseAdvancedFeatures(user)) {
      // í”„ë¦¬ë¯¸ì—„: GPT-4o ì „ì²´ ê¸°ëŠ¥
      return await langchainService.getResponse(userMessage: message);
    } else {
      // ì¼ë°˜: ê¸°ë³¸ ê¸°ëŠ¥
      return await openAIService.getChatResponse(
        message: message,
        conversationType: 'ì¼ë°˜',
      );
    }
  }
}
```

### 3. ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…

```dart
class AiServiceWrapper {
  Future<String> safeGetResponse(String message) async {
    try {
      AppLogger.info('AI ìš”ì²­ ì‹œì‘: $message');
      
      final response = await langchainService.getResponse(
        userMessage: message,
      );
      
      AppLogger.info('AI ì‘ë‹µ ì„±ê³µ: ${response.length} ë¬¸ì');
      return response;
      
    } on AppError catch (e) {
      AppLogger.error('AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜: ${e.message}');
      return 'ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
      
    } catch (e) {
      AppLogger.error('ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: $e');
      return 'ì„œë¹„ìŠ¤ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
    }
  }
}
```

### 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```dart
class PerformanceMonitor {
  static void trackApiCall(String service, int responseTime, bool success) {
    final metrics = {
      'service': service,
      'response_time_ms': responseTime,
      'success': success,
      'timestamp': DateTime.now().toIso8601String(),
    };
    
    // Analytics ì „ì†¡ (Firebase Analytics, ë“±)
    FirebaseAnalytics.instance.logEvent(
      name: 'ai_api_call',
      parameters: metrics,
    );
  }
}

// ì‚¬ìš© ì˜ˆì‹œ
final stopwatch = Stopwatch()..start();
try {
  final response = await langchainService.getResponse(userMessage: message);
  PerformanceMonitor.trackApiCall('gpt4o', stopwatch.elapsedMilliseconds, true);
} catch (e) {
  PerformanceMonitor.trackApiCall('gpt4o', stopwatch.elapsedMilliseconds, false);
}
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### 1. ì‹¤í—˜ì  ê¸°ëŠ¥
- **ë©€í‹°ëª¨ë‹¬ ì…ë ¥**: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ + ìŒì„±
- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
- **ê°œì¸í™”**: ì‚¬ìš©ìë³„ ë§ì¶¤ AI ì„±ê²©

### 2. ê³ ê¸‰ í™œìš©
- **RAG (Retrieval-Augmented Generation)**: ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ ì—°ë™
- **Fine-tuning**: í‹”ìš´ì´ ì „ìš© ëª¨ë¸ í•™ìŠµ
- **ì—ì´ì „íŠ¸**: ë³µí•© ì‘ì—… ìˆ˜í–‰ AI

### 3. ëª¨ë‹ˆí„°ë§ ê°•í™”
- **A/B í…ŒìŠ¤íŠ¸**: ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
- **ì‚¬ìš©ì í”¼ë“œë°±**: ì‘ë‹µ í’ˆì§ˆ ê°œì„ 
- **ë¹„ìš© ìµœì í™”**: íš¨ìœ¨ì ì¸ ëª¨ë¸ ì‚¬ìš©

---

ğŸš€ **ì´ì œ GPT-4oì™€ GPT-4o-audio-previewì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë”ìš± ìì—°ìŠ¤ëŸ½ê³  ë„ì›€ì´ ë˜ëŠ” AI ìƒë‹´ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

ë¬¸ì˜ì‚¬í•­ì´ë‚˜ ì¶”ê°€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ì—°ë½ ì£¼ì„¸ìš”. ğŸ¤–âœ¨
